
======================================================================
        Q-LEARNING WITH MOVING GOALS: EXPERIMENTAL REPORT
======================================================================

EXPERIMENT SETUP:
-----------------
Grid Size:              8x8
Start Position:         (0, 0)
Number of Episodes:     1000
Max Steps per Episode:  100
Number of Obstacles:    7
Possible Goals:         [(7, 7), (7, 0), (0, 7), (7, 3), (3, 7)]

AGENT CONFIGURATION:
--------------------
Learning Rate:          0.1
Discount Factor:        0.95
Initial Epsilon:        1.0
Min Epsilon:            0.01
Epsilon Decay:          0.995

RESULTS (Last 100 Episodes):
=============================

FIXED GOAL (Baseline):
----------------------
Success Rate:           100.00%
Average Reward:         8.59
Average Steps:          14.11
Q-table Size:           64 states
Goal Position:          (7, 7)

MOVING GOAL:
------------
Success Rate:           43.00%
Average Reward:         -2.72
Average Steps:          69.61
Q-table Size:           64 states

PERFORMANCE DEGRADATION:
------------------------
Success Rate Drop:      57.00%
Reward Drop:            11.31
Steps Increase:         55.50

CONCLUSIONS:
============

1. LEARNING FAILURE WITH MOVING GOALS:
   The agent with moving goals achieved a success rate of 43.0%
   compared to 100.0% with a fixed goal - a degradation of
   57.0 percentage points.

2. WHY THIS HAPPENS:
   - Q-learning assumes a stationary environment (MDP with fixed transitions/rewards)
   - When the goal changes, the optimal policy changes, making previously learned
     Q-values obsolete or misleading
   - The agent wastes time exploring states that lead to goals that are no longer active
   - Each episode with a different goal essentially "unlearns" progress from previous
     episodes with different goals

3. STATE SPACE EXPLORATION:
   The moving goal agent explored 64 states vs 64 for
   fixed goal, showing less exploration
   but no benefit in performance.

4. PRACTICAL IMPLICATIONS:
   This demonstrates that standard Q-learning is NOT suitable for:
   - Non-stationary environments
   - Multi-task learning without task identification
   - Scenarios where goals/objectives change over time
   
   Alternative approaches needed:
   - Goal-conditioned RL (HER, UVFA)
   - Meta-RL / Learning to learn
   - Context-aware policies
   - Hierarchical RL with goal abstraction

======================================================================
Report generated: Moving_Goal_Results
======================================================================
